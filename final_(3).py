# -*- coding: utf-8 -*-
"""final_(3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OlBObQej8Zck9FeKZvTqUYKZKqYl3vRY
"""

from google.colab import drive
drive.mount('/content/drive')

from zipfile import ZipFile

pip install tensorflow==2.12

import tensorflow as tf
print(tf.__version__)

import keras
print(keras.__version__)

!pip install tensorflow-addons

from tensorflow.keras import layers, Input, Model, Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, Dense, Dropout, Flatten, SeparableConv2D, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.applications import VGG19, InceptionV3
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import tensorflow as tf
print(tf.__version__)

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.image as img
#IMPORTING LIBRARIES
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt

import cv2
import itertools
import pathlib
import warnings
from PIL import Image
from random import randint
warnings.filterwarnings('ignore')

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import matthews_corrcoef as MCC
from sklearn.metrics import balanced_accuracy_score as BAS
from sklearn.metrics import classification_report, confusion_matrix

#   DataGenerator to read images and rescale images
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow import keras
from keras import layers
import tensorflow as tf
import tensorflow_addons as tfa

#   count each class samples
from collections import Counter

from tensorflow.keras.applications import VGG19
from tensorflow.keras.preprocessing import image_dataset_from_directory
from keras.utils.vis_utils import plot_model
from tensorflow.keras import Input
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Conv2D, Flatten
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG
from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, GlobalAveragePooling2D

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Concatenate
from tensorflow.keras.models import Sequential


from distutils.dir_util import copy_tree, remove_tree

#   SMOTETomek from imblance library
from imblearn.combine import SMOTETomek

import os
import tensorflow as tf

# List only directories in the specified path
directory_path = "/content/drive/MyDrive/archive"
folders = [f for f in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, f))]

print("Folders in '/content/drive/MyDrive/archive':")
print(folders)

# Print the TensorFlow version
print("TensorFlow Version:", tf.__version__)

import tensorflow as tf
print(tf.__version__)

import tensorflow
from tensorflow import keras
from keras.models import Sequential,load_model,Model
from keras.layers import Conv2D,MaxPool2D,AveragePooling2D,Dense,Flatten,ZeroPadding2D,BatchNormalization,Activation,Add,Input,Dropout,GlobalAveragePooling2D
from keras.optimizers import SGD
from keras.initializers import glorot_uniform
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau

X, y = [], []

## Images rescaling
datagen = ImageDataGenerator(rescale=1.0/255.0)

#   Load images by resizing and shuffling randomly
train_dataset = datagen.flow_from_directory("/content/drive/MyDrive/archive", target_size=(128, 128),batch_size=7000, shuffle=True)

### Seperate Dataset from  Data Genrator
X, y = train_dataset.next()

samples_before = len(X)
print("Images shape :\t", X.shape)
print("Labels shape :\t", y.shape)

#   Number of samples in classes
print("Number of samples in each class:\t", sorted(Counter(np.argmax(y, axis=1)).items()))

#   class labels as per indices
print("Classes Names according to index:\t", train_dataset.class_indices)

##random figures.

#   show some samples from the dataset randomly
import random
fig = plt.figure(figsize=(10,8))

CLASSES=['cerescopar', 'healthy', 'miner', 'phoma', 'rust']

rows = 5
columns = 5

for i in range(rows * columns):
    fig.add_subplot(rows, columns, i+1)
    num = random.randint(0, len(X)-1 )
    plt.imshow(X[num])
    plt.axis('off')
    plt.title(CLASSES[(np.argmax(y[num]))], fontsize=8)
plt.axis('off')
plt.show()

#   reshaping the images to 1D
X = X.reshape(-1, 128 * 128 * 3)

#   Oversampling method to remove imbalance class problem
X, y = SMOTETomek().fit_resample(X, y)

#   reshape images to images size of 208, 176, 3
X = X.reshape(-1, 128, 128, 3)

samples_after = len(X)
print("Number of samples after SMOTETomek :\t", sorted(Counter(np.argmax(y, axis=1)).items()))

fig = plt.figure(figsize=(10,8))

rows = 5
columns = 5

for i in range(rows * columns):
    fig.add_subplot(rows, columns, i+1)
    num = random.randint(samples_before, samples_after - 1 )
    plt.imshow(X[num])
    plt.axis('off')
    plt.title(CLASSES[(np.argmax(y[num]))], fontsize=8)
plt.axis('off')
plt.show()

## sPLITTING

#   20% split to validation and 80% split to train set
X_train, x_val, y_train, y_val = train_test_split(X,y, test_size = 0.2)

#   20% split to test from 80% of train and 60% remains in train set
X_train, x_test, y_train, y_test = train_test_split(X_train,y_train, test_size = 0.2)

# Number of samples after train test split
print("Number of samples after splitting into Training, validation & test set\n")

print("Train     \t",sorted(Counter(np.argmax(y_train, axis=1)).items()))
print("Validation\t",sorted(Counter(np.argmax(y_val, axis=1)).items()))
print("Test      \t",sorted(Counter(np.argmax(y_test, axis=1)).items()))

#   to free memeory we don't need this one as we split our data
del X, y

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input

base_model_tf=ResNet50(include_top=False,weights='imagenet',input_shape=(128,128,3),classes=5)

#Model building
base_model_tf.trainable=False

pt=Input(shape=(128,128,3))
func=tensorflow.cast(pt,tensorflow.float32)
x=preprocess_input(func) #This function used to zero-center each color channel wrt Imagenet dataset
model_resnet=base_model_tf(x,training=False)
model_resnet=GlobalAveragePooling2D()(model_resnet)
model_resnet=Dense(128,activation='relu')(model_resnet)
model_resnet=Dense(64,activation='relu')(model_resnet)
model_resnet=Dense(5,activation='softmax')(model_resnet)


model_main=Model(inputs=pt,outputs=model_resnet)
model_main.summary()


### Model Compilation

# from tensorflow.keras.optimizers import SGD

model_main.compile(
    optimizer="Adam",
    loss = tf.keras.losses.CategoricalCrossentropy(name='loss'),
    # loss = "sparse_categorical_crossentropy",
    metrics=[
        tf.keras.metrics.CategoricalAccuracy(name='acc'),
        tf.keras.metrics.AUC(name='auc'),
        tfa.metrics.F1Score(num_classes=5),
        tf.metrics.Precision(name="precision"),
        tf.metrics.Recall(name="recall") ])

from tensorflow import keras
model_main=keras.models.load_model('/content/drive/MyDrive/archive/model_f.h5')

#   declare to run on small gpu create batch sizes of images
valAug = ImageDataGenerator()

#   defining batch size
# batch_size = 8

hist = model_main.fit(valAug.flow(X_train, y_train, batch_size=64, shuffle = True),
steps_per_epoch=len(X_train)  // 80,
validation_data=valAug.flow(x_val, y_val, batch_size=64, shuffle = True),
validation_steps=len(x_test) //80,
epochs= 50,
batch_size=64,
# callbacks = CALLBACKS
)

model_path= '/content/drive/MyDrive/plan/plant_coffee2.h5'
model_main.save(model_path)

### Evaluate Model
test_scores = model_main.evaluate(x_test, y_test, batch_size = 32)


print("\n\nTesting Loss : \t\t {0:0.6f}".format(test_scores[0] ))
print("Testing Accuracy : \t {0:0.6f} %".format(test_scores[1] * 100))
print("Testing AC : \t\t {0:0.6f} %".format(test_scores[2] * 100))
print("Testing F1-Score : \t {0:0.6f} %".format(((test_scores[3][0] + test_scores[3][1] + test_scores[3][2] + test_scores[3][3] + test_scores[3][4])/5) * 100))
print("Testing Precision : \t {0:0.6f} %".format(test_scores[4] * 100))
print("Testing Recall : \t {0:0.6f} %".format(test_scores[5] * 100))

plt.plot(hist.history['acc'], 'b')
plt.plot(hist.history['val_acc'], 'g')
plt.title("Model Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["train", "val"])
plt.show()

plt.plot(hist.history['loss'], 'b')
plt.plot(hist.history['val_loss'], 'g')
plt.title("Model Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["train", "val"])
plt.show()

plt.plot(hist.history['auc'], 'b')
plt.plot(hist.history['val_auc'], 'g')
plt.title("Model AUC")
plt.xlabel("Epochs")
plt.ylabel("AUC")
plt.legend(["train", "val"])
plt.show()

plt.plot(hist.history['precision'], 'b')
plt.plot(hist.history['val_precision'], 'g')
plt.title("Model Precision")
plt.xlabel("Epochs")
plt.ylabel("Precision")
plt.legend(["train", "val"])
plt.show()

plt.plot(hist.history['recall'], 'b')
plt.plot(hist.history['val_recall'], 'g')
plt.title("Model Recall")
plt.xlabel("Epochs")
plt.ylabel("Recall")
plt.legend(["train", "val"])
plt.show()

pred_labels = model_main.predict(x_test, batch_size=32)

def roundoff(arr):
    arr[np.argwhere(arr != arr.max())] = 0
    arr[np.argwhere(arr == arr.max())] = 1
    return arr

for labels in pred_labels:
    labels = roundoff(labels)

print(classification_report(y_test, pred_labels, target_names=CLASSES))

pred_ls = np.argmax(pred_labels, axis=1)
test_ls = np.argmax(y_test, axis=1)

conf_arr = confusion_matrix(test_ls, pred_ls)

plt.figure(figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')

ax = sns.heatmap(conf_arr, cmap='Greens', annot=True, fmt='d', xticklabels= CLASSES, yticklabels=CLASSES)

plt.title('Confusion Matrix of Model', fontweight='bold', fontsize=14.0)
plt.xlabel('Predictions', fontweight='bold', fontsize=13)
plt.ylabel('Ground Truth', fontweight='bold', fontsize=13)
plt.tight_layout()
plt.show(ax)

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(4):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred_labels[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), pred_labels.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

plt.figure()
lw = 2
plt.plot(
    fpr[2],
    tpr[2],
    color="darkorange",
    lw=lw,
    label="ROC curve (area = %0.4f)" % roc_auc[2])

plt.plot([0, 1], [0, 1], color="navy", lw=lw, linestyle="--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver operating characteristic ")
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import accuracy_score

pred_labels = model_main.predict(x_test, batch_size=32)

def roundoff(arr):
    arr[np.argwhere(arr != arr.max())] = 0
    arr[np.argwhere(arr == arr.max())] = 1
    return arr

for labels in pred_labels:
    labels = roundoff(labels)

# Convert one-hot encoded predictions to integer labels
y_pred_labels = np.argmax(pred_labels, axis=1)

# Convert one-hot encoded ground truth labels to integer labels
y_true_labels = np.argmax(y_test, axis=1)

print(classification_report(y_true_labels, y_pred_labels, target_names=CLASSES))
accuracy = accuracy_score(y_true_labels, y_pred_labels)
print("Accuracy:", accuracy)



# Displaying error evolution during training (to see if the model learns or not)

acc_list = historyr.history['accuracy'] + historyr2.history['accuracy']
acc = np.array(acc_list)
plt.plot(acc, label='accuracy')
val_acc = historyr.history['val_accuracy'] + historyr2.history['val_accuracy']
plt.plot(val_acc, label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.6, 1])
plt.legend(loc='lower right')
plt.show()

acc_list = historyr.history['precision_2'] + historyr2.history['precision_3']
acc = np.array(acc_list)
plt.plot(acc, label='precision')
val_acc = historyr.history['val_precision_2'] + historyr2.history['val_precision_3']
plt.plot(val_acc, label = 'val_precision')
plt.xlabel('Epoch')
plt.ylabel('Precision')
plt.ylim([0.6, 1])
plt.legend(loc='lower right')
plt.show()

acc_list = historyr.history['recall_2'] + historyr2.history['recall_3']
acc = np.array(acc_list)
plt.plot(acc, label='recall')
val_acc = historyr.history['val_recall_2'] + historyr2.history['val_recall_3']
plt.plot(val_acc, label = 'val_recall')
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.ylim([0.6, 1])
plt.legend(loc='lower right')
plt.show()